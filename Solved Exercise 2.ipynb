{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><a href=\"http://ml-school.uni-koeln.de\">Summer School \"Deep Learning for\n",
    "    Language Analysis\"</a> <br/><strong>Text Analysis with Deep Learning</strong><br/>Sep 5 - 9, 2022<br/>Nils Reiter<br/><a href=\"mailto:nils.reiter@uni-koeln.de\">nils.reiter@uni-koeln.de</a></div>\n",
    "\n",
    "# Exercise 2\n",
    "\n",
    "This exercise is about sequence labeling. A sequence of items (words, in this case) must the tagged with a sequence of labels. In this case the labels are named entity tags in the BIO scheme.\n",
    "\n",
    "The data we will be using comes from the Groningen Meaning Bank (GMB). Its annotation scheme can be found [here](http://www.let.rug.nl/bjerva/gmb/manual.php). As always, we will first preprocess the data, and then create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit GPU memory to 4 GB\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import into a pandas dataframe, and fill missing values. Also, let's look at the head of the table. We also directly encode the strings as integers, using the [numpy-function `np.unique(...)`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html). This will allow us to convert the index numbers back into readable tag strings later on.\n",
    "\n",
    "For padding (see below), we will be using `_____` as a \"word\", and `O` as a tag. `_____` needs to be added to the lists of unique words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in CSV file\n",
    "data = pd.read_csv(\"data/ner/gmb.csv\",encoding = 'latin1')\n",
    "\n",
    "# the first column of the file contains the sentence number\n",
    "# -- but only for the first token of each sentence.\n",
    "# The following line fills the rows downwards.\n",
    "data = data.fillna(method = 'ffill')\n",
    "\n",
    "# create a list of unique words and assign an integer number to it\n",
    "unique_words, coded_words = np.unique(data[\"Word\"], return_inverse=True)\n",
    "data[\"Word_idx\"] = coded_words\n",
    "EMPTY_WORD_IDX = len(unique_words)\n",
    "np.array(unique_words.tolist().append(\"_____\"))\n",
    "num_words = len(unique_words)+1\n",
    "\n",
    "# create a list of unique tags and assign an integer number to it\n",
    "unique_tags, coded_tags = np.unique(data[\"Tag\"], return_inverse=True)\n",
    "data[\"Tag_idx\"]  = coded_tags\n",
    "NO_TAG_IDX = unique_tags.tolist().index(\"O\")\n",
    "num_words_tag = len(unique_tags)\n",
    "\n",
    "# for verification and inspection, we print out the table so far\n",
    "data[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we convert the table in such a way that we can access individual sentences. The result of the function is a list of list of tuples, with the tuples containing the word, its part of speech tag and its named entity tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(data):\n",
    "    n_sent=1\n",
    "    agg_func = lambda s:[(w,t) for w,t in zip(s[\"Word_idx\"].values.tolist(),\n",
    "                                                     s[\"Tag_idx\"].values.tolist())]\n",
    "    grouped = data.groupby(\"Sentence #\").apply(agg_func)\n",
    "    return [s for s in grouped]\n",
    "\n",
    "\n",
    "sentences = get_sentences(data)\n",
    "\n",
    "# print out the first sentence for verification\n",
    "print(sentences[0])\n",
    "\n",
    "# extract list of tokens and list of ne tags\n",
    "x = [ [ w[0] for w in s ] for s in sentences ]\n",
    "y = [ [ w[1] for w in s ] for s in sentences ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Padding\n",
    "\n",
    "Now that we have sentences and tags encoded as integer values and in individual lists, we need to make sure that every input has the same length. This is called \"padding\", and the simple solution is to extend set all sequences with a null value, so that they are of the same length.\n",
    "\n",
    "The padding can be done in two steps:\n",
    "1. Find out how long the longest sentence is (hint: list comprehension!).\n",
    "2. Use the keras function [`pad_sequences()`](https://keras.io/api/preprocessing/timeseries/) to do the padding on the x and y variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# find the maximum length for the sentences\n",
    "max_len = max([len(s) for s in x])\n",
    "\n",
    "# shorter sentences are now padded to same length, using (index of) padding symbol\n",
    "x = pad_sequences(maxlen = max_len, sequences = x, padding = 'post', value = EMPTY_WORD_IDX)\n",
    "\n",
    "# we do the same for the y data\n",
    "y = pad_sequences(maxlen = max_len, sequences = y, padding = 'post', value = NO_TAG_IDX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: One-Hot-Encoding\n",
    "\n",
    "Named entity recognition as done here is a multiclass classification problem: Each token is assigned one of more than two possible classes (`unique_tags` from before contains the list of classes). Multiclass classification problems are solved with neural networks, such that the network produces a vector of probabilities as output â€“ one probability for each class. We can then easily extract the class with the highest probability as prediction.\n",
    "\n",
    "Therefore, we need to encode our $y$ data into the same format. Luckily, keras provides a function to use here: [`to_categorical()`](https://keras.io/api/utils/python_utils/#to_categorical-function). Use it to map our output integers into vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y = np.array([to_categorical(i, num_classes = num_words_tag) for i in  y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split\n",
    "\n",
    "In contrast to our previous exercise, in which defined train and test data sets were given, we only have a single data set here. In these cases, we need to manually split the data set into training and test set. This can be done with a function from the library `scikit-learn`: [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), which yields a list of multiple outputs as return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model Architecture\n",
    "\n",
    "Now we create the model architecture. You find a simple initial architecture below. Play around with it, try to improve its performance!\n",
    "\n",
    "Things to try:\n",
    "- Pretrained embeddings\n",
    "- Bidirectionality\n",
    "- More dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape = (max_len,)))\n",
    "model.add(layers.Embedding(input_dim = num_words, output_dim = 1, input_length = max_len))\n",
    "model.add(layers.SimpleRNN(units = 5, return_sequences = True))\n",
    "model.add(layers.Dense(num_words_tag, activation = 'softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, np.array(y_train),\n",
    "    batch_size = 128,\n",
    "    epochs = 1,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation by class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have mostly looked at accuracy scores. For this task, however, this may not giving us the entire picture, because there are many different target classes, and the model might perform differently for them. So look at an evaluation by class. For this, the [function `classification_report(...)` from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_test = np.argmax(y_test, axis=2)\n",
    "\n",
    "y_pred = np.argmax(model.predict(x_test), axis=2)\n",
    "\n",
    "\n",
    "print(classification_report(Y_test.flatten(), y_pred.flatten(), zero_division=0, target_names=unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
