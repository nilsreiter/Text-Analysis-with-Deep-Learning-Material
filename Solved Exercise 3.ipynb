{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><a href=\"http://ml-school.uni-koeln.de\">Summer School \"Deep Learning for\n",
    "    Language Analysis\"</a> <br/><strong>Text Analysis with Deep Learning</strong><br/>Sep 5 - 9, 2022<br/>Nils Reiter<br/><a href=\"mailto:nils.reiter@uni-koeln.de\">nils.reiter@uni-koeln.de</a></div>\n",
    "\n",
    "# Exercise 3: Joint learning\n",
    "\n",
    "A major advantage of the functional API of keras is that it allows us to do *joint learning*: We can train a network that makes multiple predictions at the same time. In this exercise, you will extend what you have been doing in Exercise 2, in order to simultaneously predict named entities *and* parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit GPU memory to 4 GB\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We import libraries and load the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in CSV file\n",
    "data = pd.read_csv(\"data/ner/gmb.csv\", encoding = 'latin1')\n",
    "\n",
    "# the first column of the file contains the sentence number\n",
    "# -- but only for the first token of each sentence.\n",
    "# The following line fills the rows downwards.\n",
    "data = data.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Encode part of speech information\n",
    "\n",
    "In Exercise 2, we only cared about the word and named entity columns. We need to extend this, in order to also handle the part of speech column. In principle, we can handle it similarly to the named entity column, but there is no obvious tag to pad sequences with. We therefore need to add an additional dummy part of speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique words and assign an integer number to it\n",
    "unique_words, coded_words = np.unique(data[\"Word\"], return_inverse=True)\n",
    "data[\"Word_idx\"] = coded_words\n",
    "EMPTY_WORD_IDX = len(unique_words)\n",
    "np.array(unique_words.tolist().append(\"_____\"))\n",
    "num_words = len(unique_words)+1\n",
    "\n",
    "unique_pos_tags, coded_pos_tags = np.unique(data[\"POS\"], return_inverse=True)\n",
    "data[\"POS_idx\"]  = coded_pos_tags\n",
    "NO_POS_TAG_IDX = len(unique_pos_tags)\n",
    "unique_pos_tags = unique_pos_tags.tolist()\n",
    "unique_pos_tags.append(\"_\")\n",
    "unique_pos_tags = np.array(unique_pos_tags)\n",
    "num_pos_tags = len(unique_pos_tags)\n",
    "\n",
    "\n",
    "# create a list of unique tags and assign an integer number to it\n",
    "unique_ne_tags, coded_ne_tags = np.unique(data[\"Tag\"], return_inverse=True)\n",
    "data[\"NE_idx\"]  = coded_ne_tags\n",
    "NO_NE_TAG_IDX = unique_ne_tags.tolist().index(\"O\")\n",
    "num_ne_tags = len(unique_ne_tags)\n",
    "\n",
    "# for verification and inspection, we can inspect the table so far\n",
    "data[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to extend the `get_sentences()`-function, because it also needs to return the values of the part of speech index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in sentence-wise processing.\n",
    "# Therefore, we use a function that gives us individual sentences.\n",
    "def get_sentences(data):\n",
    "  n_sent=1\n",
    "  agg_func = lambda s:[(w,p,t) \n",
    "    for w,p,t in zip(\n",
    "      s[\"Word_idx\"].values.tolist(),\n",
    "      s[\"POS_idx\"].values.tolist(),\n",
    "      s[\"NE_idx\"].values.tolist())]\n",
    "  grouped = data.groupby(\"Sentence #\").apply(agg_func)\n",
    "  return [s for s in grouped]\n",
    "\n",
    "sentences = get_sentences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "As before, we have to pad the sentences to the same length. This time, we also pad the POS sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# find the maximum length for the sentences\n",
    "max_len = max([len(s) for s in sentences])\n",
    "\n",
    "# extract the word index\n",
    "x = [ [ w[0] for w in s ] for s in sentences ]\n",
    "\n",
    "# extract the tag index\n",
    "y_pos = [ [ w[1] for w in s ] for s in sentences ]\n",
    "y_ne = [ [ w[2] for w in s ] for s in sentences ]\n",
    "\n",
    "# shorter sentences are now padded to same length, using (index of) padding symbol\n",
    "x = pad_sequences(maxlen = max_len, sequences = x, \n",
    "  padding = 'post', value = EMPTY_WORD_IDX)\n",
    "\n",
    "# we do the same for the y data\n",
    "y_ne = pad_sequences(maxlen = max_len, sequences = y_ne, \n",
    "  padding = 'post', value = NO_NE_TAG_IDX)\n",
    "y_pos = pad_sequences(maxlen = max_len, sequences = y_pos, \n",
    "  padding = 'post', value = NO_POS_TAG_IDX)\n",
    "\n",
    "y_ne = np.array(y_ne)\n",
    "y_pos = np.array(y_pos)\n",
    "\n",
    "# but we also convert the indices to one-hot-encoding\n",
    "y_ne = to_categorical(y_ne, num_classes = num_ne_tags)\n",
    "y_pos = to_categorical(y_pos, num_classes = num_pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the data intro train and test, we need to do one more thing: We need to apply the same split to `y_pos` and `y_ne`. To do this, we supply a third argument to the `train_test_split()` function, namely a list of indices for the data points. These can then be applied to the `y_pos` data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dea1d0baab4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m x_train, x_test, y_ne_train, y_ne_test, train_indices, test_indices = train_test_split(x, y_ne, \n\u001b[0m\u001b[1;32m      5\u001b[0m                                                                                   range(len(x)), test_size = 0.1, random_state=1)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# split the data into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_ne_train, y_ne_test, train_indices, test_indices = train_test_split(x, y_ne, \n",
    "                                                                                  range(len(x)), test_size = 0.1, random_state=1)\n",
    "\n",
    "y_pos_train = y_pos[train_indices]\n",
    "y_pos_test = y_pos[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Network Layout\n",
    "\n",
    "Define the network using the functional API, such that it produces two outputs for each token. Keras provides [this guide](https://keras.io/guides/functional_api/) for functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "l_input = layers.Input(shape = (max_len,))\n",
    "l_embedding = layers.Embedding(input_dim = num_words, output_dim = 50, input_length = max_len)(l_input)\n",
    "l_lstm = layers.LSTM(units = 5, return_sequences = True)(l_embedding)\n",
    "l_output_ne = layers.Dense(num_ne_tags, name=\"ne\", activation = 'softmax')(l_lstm)\n",
    "l_output_pos = layers.Dense(num_pos_tags, name=\"pos\", activation = 'softmax')(l_lstm)\n",
    "\n",
    "model = models.Model(inputs = l_input, outputs=[l_output_ne, l_output_pos])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# We use a different optimizer this time\n",
    "model.compile(optimizer='Adam', \n",
    "  loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, [np.array(y_ne_train), np.array(y_pos_train)],\n",
    "    batch_size = 64,\n",
    "    epochs = 2,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, [y_ne_test, y_pos_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse one-hot-encoding for test data\n",
    "y_ne_test = np.argmax(y_ne_test, axis=2)\n",
    "y_pos_test = np.argmax(y_pos_test, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_ne_pred, y_pos_pred = model.predict(x_test)\n",
    "\n",
    "y_ne_pred = np.argmax(y_ne_pred, axis=2)\n",
    "y_pos_pred = np.argmax(y_pos_pred, axis=2)\n",
    "\n",
    "print(classification_report(y_ne_test.flatten(), y_ne_pred.flatten(), zero_division=0, target_names=unique_ne_tags))\n",
    "print(classification_report(y_pos_test.flatten(), y_pos_pred.flatten(), zero_division=0, target_names=unique_pos_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
