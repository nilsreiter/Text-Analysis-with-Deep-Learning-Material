{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><a href=\"http://ml-school.uni-koeln.de\">Virtual Summer School \"Deep Learning for\n",
    "    Language Analysis\"</a> <br/><strong>Text Analysis with Deep Learning</strong><br/>Aug 30 â€” Sep 3, 2021<br/>Nils Reiter<br/><a href=\"mailto:nils.reiter@uni-koeln.de\">nils.reiter@uni-koeln.de</a></div>\n",
    "\n",
    "# Exercise 1: Sentiment analysis as bag of words\n",
    "\n",
    "This is the first exercise for you to solve independently, but as a group of approximately three students. Feel free to contact us via [Teams](https://teams.microsoft.com/l/team/19%3ao0bSbRYzteerqETtEm1iTg_zkTAplnFrx48z7G_LUJ81%40thread.tacv2/conversations?groupId=384c7fc6-ad91-4f72-b71e-d0e70a909d35&tenantId=4982814a-1107-493e-ac2f-3356509a8687) and call us into your room if you need support. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please do not change and execute the following code block, to make sure that you're not occupying more GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit GPU memory to 4 GB\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import numpy as np\n",
    "\n",
    "def get_labels_and_texts(file, limit=100000):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    lineNumber = 0\n",
    "    for line in bz2.BZ2File(file):\n",
    "        x = line.decode(\"utf-8\")\n",
    "        labels.append(int(x[9]) - 1)\n",
    "        texts.append(x[10:].strip())\n",
    "        lineNumber = lineNumber + 1\n",
    "        if lineNumber >= limit and limit > 0:\n",
    "          break\n",
    "    return np.array(labels), texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line opens and parses the data file, which is already available in the directory `data`. The function `get_labels_and_texts(...)` is defined above. Because we are not overwriting the argument `limit`, the function only loads the first 100000 reviews -- it is usually a good idea to work with smaller samples of the data set until everything is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_texts = get_labels_and_texts('data/amazon/train.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have imported the train and test data into variables. `train_labels` contains the classes, `train_texts` the corresponding reviews. Feel free to inspect those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# shows the seventh text (note the typo!)\n",
    "train_texts[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "In this exercise, we want the input to be a document-term-matrix. I.e., each document is represented by a numeric vector. The vector contains one dimension for each token in the vocabulary, i.e., unique token in the entire (training) corpus (we will talk about more in-depth this later).\n",
    "\n",
    "As an example, consider the following matrix:\n",
    "\n",
    "| document | dog | cat | mouse | the | a | an |\n",
    "| --- | --- | --- | --- | --- | --- | --- | \n",
    "| d1 | 5 | 6 | 0 | 10 | 5 | 6 |\n",
    "| d2 | 0 | 1 | 10 | 3 | 1 | 0 |\n",
    "| ... | ... | ... | ... | ... | ... | ... | \n",
    "\n",
    "Document `d1` contains five occurrences of the word \"dog\", six of the word \"cat\" etc.\n",
    "\n",
    "Creating such a matrix requires creating a vocabulary and then counting all these words. The class `CountVectorizer()` from `scikit-learn` is exactly what we need for this. You'll find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Please use it to create a document-term matrix for the `train_texts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "The vectors for each document are now represented as sparse arrays, i.e., they are not fully realized (zeros are not stored, for instance). To make them dense, we can use the `numpy`-function `todense()`. This is also a good opportunity to limit the number of training instances for development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "We are now ready to define the neural network. Please define a neural network with \n",
    "1. an input layer with an appropriate `shape` argument\n",
    "2. an hidden layer with size 5 and activation function `sigmoid`\n",
    "3. an output layer with activation function `sigmoid`\n",
    "\n",
    "One you have successfully fitted a first model, try to change the architecture to improve its accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model `ffnn` can now be trained on the input data, using the function `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ffnn.fit(x_train, y_train, epochs=10, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Evaluation\n",
    "\n",
    "Now that the model has been trained, we can test it on held-out data. For this evaluation, the test dataset needs to undergo the same preprocessing steps as the training data.\n",
    "1. Read in the data from a file\n",
    "2. Vectorize each document, using the same vectorizer (from above)\n",
    "3. Create one matrix `x_test` that contains the input data and one array `y_test` that contains the labels.\n",
    "\n",
    "Use the Keras-function `evaluate()` on the model ([documentation](https://keras.io/api/models/model_training_apis/#evaluate-method))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "This notebook is based on [this one](https://www.kaggle.com/muonneutrino/sentiment-analysis-with-amazon-reviews) by MuonNeutrino on kaggle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
